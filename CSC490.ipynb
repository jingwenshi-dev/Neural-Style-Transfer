{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "authorship_tag": "ABX9TyMT18TouV+ADhxvJGmun5xh"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "j7841G65g5ok",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1697400669215,
     "user_tz": 240,
     "elapsed": 11491,
     "user": {
      "displayName": "Daniel",
      "userId": "04731478159618393216"
     }
    },
    "ExecuteTime": {
     "end_time": "2023-10-23T14:35:13.648381Z",
     "start_time": "2023-10-23T14:35:13.640389Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torchvision.models import VGG19_Weights"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def preprocess_image(image_path, target_size=(224, 224)):\n",
    "    # Open the image using PIL\n",
    "    img = Image.open(image_path)\n",
    "\n",
    "    # Resize the image to the target size\n",
    "    img = img.resize(target_size)\n",
    "\n",
    "    # Convert the image to RGB (in case it's grayscale or has an alpha channel)\n",
    "    img = img.convert('RGB')\n",
    "\n",
    "    # Convert the image to a tensor\n",
    "    img_tensor = transforms.ToTensor()(img)\n",
    "\n",
    "    # Normalize the tensor\n",
    "    img_tensor = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(img_tensor)\n",
    "\n",
    "    # Add a batch dimension\n",
    "    img_tensor = img_tensor.unsqueeze(0)\n",
    "\n",
    "    return img_tensor\n",
    "\n",
    "\n",
    "def deprocess_image(x):\n",
    "    x = x.squeeze().transpose(1, 2, 0)  # Remove .cpu() here\n",
    "    x[:, :, 0] = x[:, :, 0] * 0.229 + 0.485\n",
    "    x[:, :, 1] = x[:, :, 1] * 0.224 + 0.456\n",
    "    x[:, :, 2] = x[:, :, 2] * 0.225 + 0.406\n",
    "    return x\n"
   ],
   "metadata": {
    "id": "G5xt3yQS1nQj",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1697400725395,
     "user_tz": 240,
     "elapsed": 103,
     "user": {
      "displayName": "Daniel",
      "userId": "04731478159618393216"
     }
    },
    "ExecuteTime": {
     "end_time": "2023-10-23T14:35:13.648778Z",
     "start_time": "2023-10-23T14:35:13.644947Z"
    }
   },
   "execution_count": 26,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def content_loss(base_content, target):\n",
    "    return torch.mean((base_content - target) ** 2)\n",
    "\n",
    "def gram_matrix(input_tensor):\n",
    "    result = torch.einsum('bijc,bijd->bcd', input_tensor, input_tensor)\n",
    "    b, i, j, c = input_tensor.size()\n",
    "    num_locations = b * i * j\n",
    "    return result / num_locations\n",
    "\n",
    "def style_loss(base_style, gram_target):\n",
    "    gram_style = gram_matrix(base_style)\n",
    "    return torch.mean((gram_style - gram_target) ** 2)"
   ],
   "metadata": {
    "id": "Ic3mz_jz1oQw",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1697400726831,
     "user_tz": 240,
     "elapsed": 102,
     "user": {
      "displayName": "Daniel",
      "userId": "04731478159618393216"
     }
    },
    "ExecuteTime": {
     "end_time": "2023-10-23T14:35:13.663164Z",
     "start_time": "2023-10-23T14:35:13.647289Z"
    }
   },
   "execution_count": 27,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class VGG19(nn.Module):\n",
    "    def __init__(self, input_shape=(3, 224, 224)):\n",
    "        super(VGG19, self).__init__()\n",
    "\n",
    "        # Block 1\n",
    "        self.block1_conv1 = nn.Conv2d(input_shape[0], 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.block1_conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.block1_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Block 2\n",
    "        self.block2_conv1 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.block2_conv2 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.block2_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Block 3\n",
    "        self.block3_conv1 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.block3_conv2 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.block3_conv3 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.block3_conv4 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.block3_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Block 4\n",
    "        self.block4_conv1 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.block4_conv2 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.block4_conv3 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.block4_conv4 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.block4_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Block 5\n",
    "        self.block5_conv1 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.block5_conv2 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.block5_conv3 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.block5_conv4 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.block5_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Top layers (Fully connected)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(512*7*7, 4096)\n",
    "        self.fc2 = nn.Linear(4096, 4096)\n",
    "        self.predictions = nn.Linear(4096, 1000)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Block 1\n",
    "        x = F.relu(self.block1_conv1(x))\n",
    "        x = F.relu(self.block1_conv2(x))\n",
    "        x = self.block1_pool(x)\n",
    "\n",
    "        # Block 2\n",
    "        x = F.relu(self.block2_conv1(x))\n",
    "        x = F.relu(self.block2_conv2(x))\n",
    "        x = self.block2_pool(x)\n",
    "\n",
    "        # Block 3\n",
    "        x = F.relu(self.block3_conv1(x))\n",
    "        x = F.relu(self.block3_conv2(x))\n",
    "        x = F.relu(self.block3_conv3(x))\n",
    "        x = F.relu(self.block3_conv4(x))\n",
    "        x = self.block3_pool(x)\n",
    "\n",
    "        # Block 4\n",
    "        x = F.relu(self.block4_conv1(x))\n",
    "        x = F.relu(self.block4_conv2(x))\n",
    "        x = F.relu(self.block4_conv3(x))\n",
    "        x = F.relu(self.block4_conv4(x))\n",
    "        x = self.block4_pool(x)\n",
    "\n",
    "        # Block 5\n",
    "        x = F.relu(self.block5_conv1(x))\n",
    "        x = F.relu(self.block5_conv2(x))\n",
    "        x = F.relu(self.block5_conv3(x))\n",
    "        x = F.relu(self.block5_conv4(x))\n",
    "        x = self.block5_pool(x)\n",
    "\n",
    "        # Top layers (Fully connected)\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.predictions(x)"
   ],
   "metadata": {
    "id": "Od5VWJJh1rCJ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1697400727998,
     "user_tz": 240,
     "elapsed": 211,
     "user": {
      "displayName": "Daniel",
      "userId": "04731478159618393216"
     }
    },
    "ExecuteTime": {
     "end_time": "2023-10-23T14:35:13.669242Z",
     "start_time": "2023-10-23T14:35:13.656191Z"
    }
   },
   "execution_count": 28,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "# Define constants\n",
    "CONTENT_LAYER = 'block4_conv2'\n",
    "STYLE_LAYERS = ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1', 'block5_conv1']\n",
    "\n",
    "def get_content_and_style_models(vgg_model, content_layer = CONTENT_LAYER, style_layers = STYLE_LAYERS):\n",
    "\n",
    "    # Extracting content layer\n",
    "    content_model = nn.Sequential(*[getattr(vgg_model, layer) for layer in list(vgg_model._modules.keys())[:list(vgg_model._modules.keys()).index(content_layer) + 1]])\n",
    "\n",
    "    # Extracting style layers\n",
    "    style_models = []\n",
    "    for style_layer in style_layers:\n",
    "        style_model = nn.Sequential(*[getattr(vgg_model, layer) for layer in list(vgg_model._modules.keys())[:list(vgg_model._modules.keys()).index(style_layer) + 1]])\n",
    "        style_models.append(style_model)\n",
    "\n",
    "    return content_model, style_models"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-23T14:35:13.669501Z",
     "start_time": "2023-10-23T14:35:13.660596Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def compute_loss_and_gradients(combined_image, content_model, style_models, content_target, style_targets, content_weight, style_weight):\n",
    "    combined_image.requires_grad_(True)\n",
    "\n",
    "    # Get current content and style of the combined image\n",
    "    current_content = content_model(combined_image)\n",
    "    current_styles = [style_model(combined_image) for style_model in style_models]\n",
    "\n",
    "    # Content loss\n",
    "    c_loss = content_loss(current_content, content_target)\n",
    "\n",
    "    # Style loss\n",
    "    s_loss = sum([style_loss(current_styles[i], style_targets[i]) for i in range(len(STYLE_LAYERS))])\n",
    "\n",
    "    # Total loss\n",
    "    total_loss = content_weight * c_loss + style_weight * s_loss\n",
    "\n",
    "    # Backward pass\n",
    "    total_loss.backward(retain_graph=True)\n",
    "\n",
    "    return total_loss, combined_image.grad\n"
   ],
   "metadata": {
    "id": "EFYYbENy1zWs",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1697400789294,
     "user_tz": 240,
     "elapsed": 111,
     "user": {
      "displayName": "Daniel",
      "userId": "04731478159618393216"
     }
    },
    "ExecuteTime": {
     "end_time": "2023-10-23T14:35:13.670013Z",
     "start_time": "2023-10-23T14:35:13.662512Z"
    }
   },
   "execution_count": 30,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "print(device)\n",
    "\n",
    "def neural_style_transfer(content_image_path, style_image_path, iterations=1000, content_weight=1e3, style_weight=1.0):\n",
    "\n",
    "    # Create the VGG19 model and move it to the device\n",
    "    vgg19_model = VGG19().to(device)\n",
    "    content_model, style_models = get_content_and_style_models(torchvision.models.vgg19(weights=VGG19_Weights.DEFAULT).features.to(device))\n",
    "\n",
    "    # Preprocess the images and move them to the device\n",
    "    content_image = preprocess_image(content_image_path).to(device)\n",
    "    style_image = preprocess_image(style_image_path).to(device)\n",
    "\n",
    "    # Get the content and style targets\n",
    "    content_target = content_model(content_image)\n",
    "    style_targets = [gram_matrix(style_model(style_image)) for style_model in style_models]\n",
    "\n",
    "    # Initialize combined image with the content image and move it to the device\n",
    "    combined_image = content_image.clone().detach().requires_grad_(True).to(device)\n",
    "\n",
    "    # Optimizer\n",
    "    opt = optim.Adam([combined_image], lr=0.02)\n",
    "\n",
    "    for i in range(iterations):\n",
    "        opt.zero_grad()\n",
    "        loss, grads = compute_loss_and_gradients(combined_image, content_model, style_models, content_target, style_targets, content_weight, style_weight)\n",
    "        opt.step()\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Iteration {i}/{iterations}: loss={loss.item()}\")\n",
    "\n",
    "    return deprocess_image(combined_image.detach().cpu().numpy())"
   ],
   "metadata": {
    "id": "_w9jakOt11bp",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1697400791108,
     "user_tz": 240,
     "elapsed": 82,
     "user": {
      "displayName": "Daniel",
      "userId": "04731478159618393216"
     }
    },
    "ExecuteTime": {
     "end_time": "2023-10-23T14:35:13.670268Z",
     "start_time": "2023-10-23T14:35:13.667734Z"
    }
   },
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Example usage:\n",
    "output_image = neural_style_transfer(\"./white.jpg\", \"./fangao.jpeg\", iterations=1000, content_weight=1, style_weight=1000000)\n",
    "plt.imshow((output_image * 255).astype(np.uint8))   \n",
    "plt.axis('off')\n",
    "plt.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "id": "qhrsCd_x13Ms",
    "executionInfo": {
     "status": "error",
     "timestamp": 1697400917676,
     "user_tz": 240,
     "elapsed": 1376,
     "user": {
      "displayName": "Daniel",
      "userId": "04731478159618393216"
     }
    },
    "outputId": "c94e2948-00c5-411e-cc0d-cce755d3c6ca",
    "ExecuteTime": {
     "end_time": "2023-10-23T14:37:19.595899Z",
     "start_time": "2023-10-23T14:36:36.002254Z"
    }
   },
   "execution_count": 33,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /Users/jingwenshi/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n",
      "4.3%IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "12.7%IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "22.5%IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "31.5%IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "41.4%IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "49.4%IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "56.9%IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "65.1%IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "71.5%IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "77.2%IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "79.6%IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "86.3%IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "92.5%IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100.0%\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "'block4_conv2' is not in list",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[33], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Example usage:\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m output_image \u001B[38;5;241m=\u001B[39m \u001B[43mneural_style_transfer\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m./white.jpg\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m./fangao.jpeg\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43miterations\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1000\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcontent_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstyle_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1000000\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      3\u001B[0m plt\u001B[38;5;241m.\u001B[39mimshow((output_image \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m255\u001B[39m)\u001B[38;5;241m.\u001B[39mastype(np\u001B[38;5;241m.\u001B[39muint8))   \n\u001B[1;32m      4\u001B[0m plt\u001B[38;5;241m.\u001B[39maxis(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124moff\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "Cell \u001B[0;32mIn[31], line 10\u001B[0m, in \u001B[0;36mneural_style_transfer\u001B[0;34m(content_image_path, style_image_path, iterations, content_weight, style_weight)\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mneural_style_transfer\u001B[39m(content_image_path, style_image_path, iterations\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1000\u001B[39m, content_weight\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1e3\u001B[39m, style_weight\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1.0\u001B[39m):\n\u001B[1;32m      7\u001B[0m \n\u001B[1;32m      8\u001B[0m     \u001B[38;5;66;03m# Create the VGG19 model and move it to the device\u001B[39;00m\n\u001B[1;32m      9\u001B[0m     vgg19_model \u001B[38;5;241m=\u001B[39m VGG19()\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m---> 10\u001B[0m     content_model, style_models \u001B[38;5;241m=\u001B[39m \u001B[43mget_content_and_style_models\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtorchvision\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodels\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvgg19\u001B[49m\u001B[43m(\u001B[49m\u001B[43mweights\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mVGG19_Weights\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mDEFAULT\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfeatures\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     12\u001B[0m     \u001B[38;5;66;03m# Preprocess the images and move them to the device\u001B[39;00m\n\u001B[1;32m     13\u001B[0m     content_image \u001B[38;5;241m=\u001B[39m preprocess_image(content_image_path)\u001B[38;5;241m.\u001B[39mto(device)\n",
      "Cell \u001B[0;32mIn[29], line 8\u001B[0m, in \u001B[0;36mget_content_and_style_models\u001B[0;34m(vgg_model, content_layer, style_layers)\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_content_and_style_models\u001B[39m(vgg_model, content_layer \u001B[38;5;241m=\u001B[39m CONTENT_LAYER, style_layers \u001B[38;5;241m=\u001B[39m STYLE_LAYERS):\n\u001B[1;32m      6\u001B[0m \n\u001B[1;32m      7\u001B[0m     \u001B[38;5;66;03m# Extracting content layer\u001B[39;00m\n\u001B[0;32m----> 8\u001B[0m     content_model \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mSequential(\u001B[38;5;241m*\u001B[39m[\u001B[38;5;28mgetattr\u001B[39m(vgg_model, layer) \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(vgg_model\u001B[38;5;241m.\u001B[39m_modules\u001B[38;5;241m.\u001B[39mkeys())[:\u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mvgg_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_modules\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkeys\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindex\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcontent_layer\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m]])\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;66;03m# Extracting style layers\u001B[39;00m\n\u001B[1;32m     11\u001B[0m     style_models \u001B[38;5;241m=\u001B[39m []\n",
      "\u001B[0;31mValueError\u001B[0m: 'block4_conv2' is not in list"
     ]
    }
   ]
  }
 ]
}
